#u <- runif(length(x), min = 0, max = 1)
#x_boot <- res[[1]][[2]][sapply(u, function(x) which.min(abs(x  - res[[1]][[1]])))]
samp_x <-   rmultinom(1, size = n_data, probs)
x_boot <- NULL
for(j in 1:length(probs)){
x_boot <- c(x_boot, rep(res[[1]][[2]][j],samp_x[j]))
}
boot_cdf <- dpCDF(x_boot, lower_bound, upper_bound, epsilon, granularity, cdp, num_trials = 1)
#boot_dist[[i]] <- boot_cdf[[1]][[1]]
boot_dist[[i]] <- gpava(boot_cdf[[1]][[2]], boot_cdf[[1]][[1]])$x
# lines(res[[1]][[2]], boot_dist[[i]], col = viridis::viridis(3, 0.3)[2])
#pava_cdf <- boot_cdf
#pava_cdf[[1]][[1]] <- gpava(boot_cdf[[1]][[2]], boot_cdf[[1]][[1]])$x
#pava_dist[[i]] <- pava_cdf[[1]][[1]]
#lines(pava_cdf[[1]][[2]], pava_cdf[[1]][[1]], col = viridis::viridis(3, 0.3)[2])
}
# pava_mat <- do.call(cbind, pava_dist)
#
# pivot_cis <- t(apply(pivot_mat, 1, quantile, c(0.025, 0.975)))
#curve(pnorm, -4, 4, n = length(res[[1]][[1]]), add = T, col = viridis::viridis(2)[2])
boot_mat <- do.call(cbind, boot_dist)
boot_cis <- t(apply(boot_mat, 1, quantile, c(0.025, 0.975)))
# lines(res[[1]][[2]], boot_cis[,1], lty = "dashed", col = "red")
# lines(res[[1]][[2]], boot_cis[,2], lty = "dashed", col = "red")
boot_cis_sim <- t(apply(boot_mat, 1, quantile, c((1-0.95^(1/length(res[[1]][[1]])))/2, 1-(1-0.95^(1/length(res[[1]][[1]])))/2)))
# lines(res[[1]][[2]], boot_cis_sim[,1], lty = "dashed", col = "orange")
# lines(res[[1]][[2]], boot_cis_sim[,2], lty = "dashed", col = "orange")
#
# lapply(res, function(x) lines(x[[2]], x[[1]], col = viridis::viridis(3, 1)[1]))
#cu$y
coverage_points <- sum(cu$y >= boot_cis[,1] & cu$y <= boot_cis[,2])/nrow(boot_cis)
coverage_sim <- all((cu$y >= boot_cis_sim[,1] & cu$y <= boot_cis_sim[,2]))
# First entry will always be 0 and last 1, so let's discard it for the coverage calculation
coverage_sim_adj <- all((cu$y >= boot_cis_sim[,1] & cu$y <= boot_cis_sim[,2])[2:(nrow(boot_cis_sim)-1)])
cat("\n N: ", n_data, " Epsilon: ", epsilon, " Granularity: ", granularity, " Bound: ",upper_bound, " Bins: ", length(res[[1]][[1]]),  "\n", "Point coverage: ", coverage_points, " Sim Coverage: ", coverage_sim_adj, "\n")
res_list[[rep]] <- list(coverage_points, coverage_sim, coverage_sim_adj)
cat("Repetition: ", rep, "\n")
}
# Set number of data points
n_data <- 1000
# Set hyperparameters for dp cdf
upper_bound <- 4
lower_bound <- -4
granularity <- 0.05
cdp <- TRUE
epsilon <- Inf
B <- 1000
# Set number of repetitions of experiment
n_rep <- 100
#Set up empty list for results
res_list <- vector("list", n_rep)
for(rep in 1:n_rep){
x <- rnorm(n_data)
if(epsilon == Inf) {
lower_bound <- min(x)
upper_bound <- max(x)
}
res <- dpCDF(x, lower_bound, upper_bound, epsilon, granularity, cdp, num_trials = 1)
# True CDF for standard normal data
cu <- curve(pnorm, lower_bound, upper_bound, n = length(res[[1]][[1]]), add = T, col = "yellow")
#par(mfrow = c(2, 2))
plot(res[[1]][[2]], res[[1]][[1]], type = "n", ylab = "Cumulative Probability", xlab = "x", bty = "n", las = 1, main = paste0("Epsilon: ", epsilon))
lapply(res, function(x) lines(x[[2]], x[[1]], col = viridis::viridis(3, 1)[1]))
pava <- gpava(res[[1]][[2]], res[[1]][[1]])
res[[1]][[1]]<- pava$x
lines(res[[1]][[2]], pava$x, col = viridis::viridis(3, 1)[3])
#
#
probs <- c(0, diff(res[[1]][[1]]))
boot_dist <- vector("list", B)
#pava_dist <- vector("list", B)
for(i in 1:B){
#x_boot <- sample(x, length(x), replace = T)
#u <- runif(length(x), min = 0, max = 1)
#x_boot <- res[[1]][[2]][sapply(u, function(x) which.min(abs(x  - res[[1]][[1]])))]
samp_x <-   rmultinom(1, size = n_data, probs)
x_boot <- NULL
for(j in 1:length(probs)){
x_boot <- c(x_boot, rep(res[[1]][[2]][j],samp_x[j]))
}
boot_cdf <- dpCDF(x_boot, lower_bound, upper_bound, epsilon, granularity, cdp, num_trials = 1)
#boot_dist[[i]] <- boot_cdf[[1]][[1]]
boot_dist[[i]] <- gpava(boot_cdf[[1]][[2]], boot_cdf[[1]][[1]])$x
# lines(res[[1]][[2]], boot_dist[[i]], col = viridis::viridis(3, 0.3)[2])
#pava_cdf <- boot_cdf
#pava_cdf[[1]][[1]] <- gpava(boot_cdf[[1]][[2]], boot_cdf[[1]][[1]])$x
#pava_dist[[i]] <- pava_cdf[[1]][[1]]
#lines(pava_cdf[[1]][[2]], pava_cdf[[1]][[1]], col = viridis::viridis(3, 0.3)[2])
}
# pava_mat <- do.call(cbind, pava_dist)
#
# pivot_cis <- t(apply(pivot_mat, 1, quantile, c(0.025, 0.975)))
#curve(pnorm, -4, 4, n = length(res[[1]][[1]]), add = T, col = viridis::viridis(2)[2])
boot_mat <- do.call(cbind, boot_dist)
boot_cis <- t(apply(boot_mat, 1, quantile, c(0.025, 0.975)))
# lines(res[[1]][[2]], boot_cis[,1], lty = "dashed", col = "red")
# lines(res[[1]][[2]], boot_cis[,2], lty = "dashed", col = "red")
boot_cis_sim <- t(apply(boot_mat, 1, quantile, c((1-0.95^(1/length(res[[1]][[1]])))/2, 1-(1-0.95^(1/length(res[[1]][[1]])))/2)))
# lines(res[[1]][[2]], boot_cis_sim[,1], lty = "dashed", col = "orange")
# lines(res[[1]][[2]], boot_cis_sim[,2], lty = "dashed", col = "orange")
#
# lapply(res, function(x) lines(x[[2]], x[[1]], col = viridis::viridis(3, 1)[1]))
#cu$y
coverage_points <- sum(cu$y >= boot_cis[,1] & cu$y <= boot_cis[,2])/nrow(boot_cis)
coverage_sim <- all((cu$y >= boot_cis_sim[,1] & cu$y <= boot_cis_sim[,2]))
# First entry will always be 0 and last 1, so let's discard it for the coverage calculation
coverage_sim_adj <- all((cu$y >= boot_cis_sim[,1] & cu$y <= boot_cis_sim[,2])[2:(nrow(boot_cis_sim)-1)])
cat("\n N: ", n_data, " Epsilon: ", epsilon, " Granularity: ", granularity, " Bound: ",upper_bound, " Bins: ", length(res[[1]][[1]]),  "\n", "Point coverage: ", coverage_points, " Sim Coverage: ", coverage_sim_adj, "\n")
res_list[[rep]] <- list(coverage_points, coverage_sim, coverage_sim_adj)
cat("Repetition: ", rep, "\n")
}
granularity <- 0.01
cdp <- TRUE
epsilon <- Inf
B <- 1000
# Set number of repetitions of experiment
n_rep <- 100
#Set up empty list for results
res_list <- vector("list", n_rep)
for(rep in 1:n_rep){
x <- rnorm(n_data)
if(epsilon == Inf) {
lower_bound <- min(x)
upper_bound <- max(x)
}
res <- dpCDF(x, lower_bound, upper_bound, epsilon, granularity, cdp, num_trials = 1)
# True CDF for standard normal data
cu <- curve(pnorm, lower_bound, upper_bound, n = length(res[[1]][[1]]), add = T, col = "yellow")
#par(mfrow = c(2, 2))
plot(res[[1]][[2]], res[[1]][[1]], type = "n", ylab = "Cumulative Probability", xlab = "x", bty = "n", las = 1, main = paste0("Epsilon: ", epsilon))
lapply(res, function(x) lines(x[[2]], x[[1]], col = viridis::viridis(3, 1)[1]))
pava <- gpava(res[[1]][[2]], res[[1]][[1]])
res[[1]][[1]]<- pava$x
lines(res[[1]][[2]], pava$x, col = viridis::viridis(3, 1)[3])
#
#
probs <- c(0, diff(res[[1]][[1]]))
boot_dist <- vector("list", B)
#pava_dist <- vector("list", B)
for(i in 1:B){
#x_boot <- sample(x, length(x), replace = T)
#u <- runif(length(x), min = 0, max = 1)
#x_boot <- res[[1]][[2]][sapply(u, function(x) which.min(abs(x  - res[[1]][[1]])))]
samp_x <-   rmultinom(1, size = n_data, probs)
x_boot <- NULL
for(j in 1:length(probs)){
x_boot <- c(x_boot, rep(res[[1]][[2]][j],samp_x[j]))
}
boot_cdf <- dpCDF(x_boot, lower_bound, upper_bound, epsilon, granularity, cdp, num_trials = 1)
#boot_dist[[i]] <- boot_cdf[[1]][[1]]
boot_dist[[i]] <- gpava(boot_cdf[[1]][[2]], boot_cdf[[1]][[1]])$x
# lines(res[[1]][[2]], boot_dist[[i]], col = viridis::viridis(3, 0.3)[2])
#pava_cdf <- boot_cdf
#pava_cdf[[1]][[1]] <- gpava(boot_cdf[[1]][[2]], boot_cdf[[1]][[1]])$x
#pava_dist[[i]] <- pava_cdf[[1]][[1]]
#lines(pava_cdf[[1]][[2]], pava_cdf[[1]][[1]], col = viridis::viridis(3, 0.3)[2])
}
# pava_mat <- do.call(cbind, pava_dist)
#
# pivot_cis <- t(apply(pivot_mat, 1, quantile, c(0.025, 0.975)))
#curve(pnorm, -4, 4, n = length(res[[1]][[1]]), add = T, col = viridis::viridis(2)[2])
boot_mat <- do.call(cbind, boot_dist)
boot_cis <- t(apply(boot_mat, 1, quantile, c(0.025, 0.975)))
# lines(res[[1]][[2]], boot_cis[,1], lty = "dashed", col = "red")
# lines(res[[1]][[2]], boot_cis[,2], lty = "dashed", col = "red")
boot_cis_sim <- t(apply(boot_mat, 1, quantile, c((1-0.95^(1/length(res[[1]][[1]])))/2, 1-(1-0.95^(1/length(res[[1]][[1]])))/2)))
# lines(res[[1]][[2]], boot_cis_sim[,1], lty = "dashed", col = "orange")
# lines(res[[1]][[2]], boot_cis_sim[,2], lty = "dashed", col = "orange")
#
# lapply(res, function(x) lines(x[[2]], x[[1]], col = viridis::viridis(3, 1)[1]))
#cu$y
coverage_points <- sum(cu$y >= boot_cis[,1] & cu$y <= boot_cis[,2])/nrow(boot_cis)
coverage_sim <- all((cu$y >= boot_cis_sim[,1] & cu$y <= boot_cis_sim[,2]))
# First entry will always be 0 and last 1, so let's discard it for the coverage calculation
coverage_sim_adj <- all((cu$y >= boot_cis_sim[,1] & cu$y <= boot_cis_sim[,2])[2:(nrow(boot_cis_sim)-1)])
cat("\n N: ", n_data, " Epsilon: ", epsilon, " Granularity: ", granularity, " Bound: ",upper_bound, " Bins: ", length(res[[1]][[1]]),  "\n", "Point coverage: ", coverage_points, " Sim Coverage: ", coverage_sim_adj, "\n")
res_list[[rep]] <- list(coverage_points, coverage_sim, coverage_sim_adj)
cat("Repetition: ", rep, "\n")
}
epsilon_settings <- c(0.5, 1, Inf)
granularity_settings <- c(0.1, 0.01, 0.005)
bound_settings <- c(4, 5)
n_settings <- c(10, 100, 1000)
settings <- expand.grid(epsilon_settings, granularity_settings, bound_settings, n_settings)
settings
# Set number of data points
n_data <- 1000
# Set hyperparameters for dp cdf
upper_bound <- 4
lower_bound <- -4
granularity <- 0.01
cdp <- TRUE
epsilon <- Inf
B <- 1000
# Set number of repetitions of experiment
n_rep <- 100
#Set up empty list for results
res_list <- vector("list", n_rep)
x <- c(rnorm(n_data/2, -1, 1), rnorm(n_data/2, 1, 1))
if(epsilon == Inf) {
lower_bound <- min(x)
upper_bound <- max(x)
}
res <- dpCDF(x, lower_bound, upper_bound, epsilon, granularity, cdp, num_trials = 1)
# True CDF for standard normal data
cu <- curve(pnorm, lower_bound, upper_bound, n = length(res[[1]][[1]]), add = T, col = "yellow")
#par(mfrow = c(2, 2))
plot(res[[1]][[2]], res[[1]][[1]], type = "n", ylab = "Cumulative Probability", xlab = "x", bty = "n", las = 1, main = paste0("Epsilon: ", epsilon))
lapply(res, function(x) lines(x[[2]], x[[1]], col = viridis::viridis(3, 1)[1]))
pava <- gpava(res[[1]][[2]], res[[1]][[1]])
res[[1]][[1]]<- pava$x
lines(res[[1]][[2]], pava$x, col = viridis::viridis(3, 1)[3])
x <- c(rnorm(n_data/2, -1, 0.1), rnorm(n_data/2, 1, 0.1))
res <- dpCDF(x, lower_bound, upper_bound, epsilon, granularity, cdp, num_trials = 1)
# True CDF for standard normal data
cu <- curve(pnorm, lower_bound, upper_bound, n = length(res[[1]][[1]]), add = T, col = "yellow")
#par(mfrow = c(2, 2))
plot(res[[1]][[2]], res[[1]][[1]], type = "n", ylab = "Cumulative Probability", xlab = "x", bty = "n", las = 1, main = paste0("Epsilon: ", epsilon))
lapply(res, function(x) lines(x[[2]], x[[1]], col = viridis::viridis(3, 1)[1]))
pava <- gpava(res[[1]][[2]], res[[1]][[1]])
res[[1]][[1]]<- pava$x
lines(res[[1]][[2]], pava$x, col = viridis::viridis(3, 1)[3])
probs <- c(0, diff(res[[1]][[1]]))
boot_dist <- vector("list", B)
#pava_dist <- vector("list", B)
for(i in 1:B){
#x_boot <- sample(x, length(x), replace = T)
#u <- runif(length(x), min = 0, max = 1)
#x_boot <- res[[1]][[2]][sapply(u, function(x) which.min(abs(x  - res[[1]][[1]])))]
samp_x <-   rmultinom(1, size = n_data, probs)
x_boot <- NULL
for(j in 1:length(probs)){
x_boot <- c(x_boot, rep(res[[1]][[2]][j],samp_x[j]))
}
boot_cdf <- dpCDF(x_boot, lower_bound, upper_bound, epsilon, granularity, cdp, num_trials = 1)
#boot_dist[[i]] <- boot_cdf[[1]][[1]]
boot_dist[[i]] <- gpava(boot_cdf[[1]][[2]], boot_cdf[[1]][[1]])$x
lines(res[[1]][[2]], boot_dist[[i]], col = viridis::viridis(3, 0.3)[2])
#pava_cdf <- boot_cdf
#pava_cdf[[1]][[1]] <- gpava(boot_cdf[[1]][[2]], boot_cdf[[1]][[1]])$x
#pava_dist[[i]] <- pava_cdf[[1]][[1]]
#lines(pava_cdf[[1]][[2]], pava_cdf[[1]][[1]], col = viridis::viridis(3, 0.3)[2])
}
# Set hyperparameters for dp cdf
upper_bound <- 4
lower_bound <- -4
granularity <- 0.01
cdp <- TRUE
epsilon <- 1
B <- 1000
# Set number of repetitions of experiment
n_rep <- 100
#Set up empty list for results
res_list <- vector("list", n_rep)
res <- dpCDF(x, lower_bound, upper_bound, epsilon, granularity, cdp, num_trials = 1)
# True CDF for standard normal data
cu <- curve(pnorm, lower_bound, upper_bound, n = length(res[[1]][[1]]), add = T, col = "yellow")
#par(mfrow = c(2, 2))
plot(res[[1]][[2]], res[[1]][[1]], type = "n", ylab = "Cumulative Probability", xlab = "x", bty = "n", las = 1, main = paste0("Epsilon: ", epsilon))
lapply(res, function(x) lines(x[[2]], x[[1]], col = viridis::viridis(3, 1)[1]))
pava <- gpava(res[[1]][[2]], res[[1]][[1]])
res[[1]][[1]]<- pava$x
lines(res[[1]][[2]], pava$x, col = viridis::viridis(3, 1)[3])
probs <- c(0, diff(res[[1]][[1]]))
boot_dist <- vector("list", B)
#pava_dist <- vector("list", B)
for(i in 1:B){
#x_boot <- sample(x, length(x), replace = T)
#u <- runif(length(x), min = 0, max = 1)
#x_boot <- res[[1]][[2]][sapply(u, function(x) which.min(abs(x  - res[[1]][[1]])))]
samp_x <-   rmultinom(1, size = n_data, probs)
x_boot <- NULL
for(j in 1:length(probs)){
x_boot <- c(x_boot, rep(res[[1]][[2]][j],samp_x[j]))
}
boot_cdf <- dpCDF(x_boot, lower_bound, upper_bound, epsilon, granularity, cdp, num_trials = 1)
#boot_dist[[i]] <- boot_cdf[[1]][[1]]
boot_dist[[i]] <- gpava(boot_cdf[[1]][[2]], boot_cdf[[1]][[1]])$x
lines(res[[1]][[2]], boot_dist[[i]], col = viridis::viridis(3, 0.3)[2])
#pava_cdf <- boot_cdf
#pava_cdf[[1]][[1]] <- gpava(boot_cdf[[1]][[2]], boot_cdf[[1]][[1]])$x
#pava_dist[[i]] <- pava_cdf[[1]][[1]]
#lines(pava_cdf[[1]][[2]], pava_cdf[[1]][[1]], col = viridis::viridis(3, 0.3)[2])
}
x <- c(rnorm(n_data/2, -1, 0.5), rnorm(n_data/2, 1, 0.5))
res <- dpCDF(x, lower_bound, upper_bound, epsilon, granularity, cdp, num_trials = 1)
# True CDF for standard normal data
cu <- curve(pnorm, lower_bound, upper_bound, n = length(res[[1]][[1]]), add = T, col = "yellow")
#par(mfrow = c(2, 2))
plot(res[[1]][[2]], res[[1]][[1]], type = "n", ylab = "Cumulative Probability", xlab = "x", bty = "n", las = 1, main = paste0("Epsilon: ", epsilon))
lapply(res, function(x) lines(x[[2]], x[[1]], col = viridis::viridis(3, 1)[1]))
pava <- gpava(res[[1]][[2]], res[[1]][[1]])
res[[1]][[1]]<- pava$x
lines(res[[1]][[2]], pava$x, col = viridis::viridis(3, 1)[3])
probs <- c(0, diff(res[[1]][[1]]))
boot_dist <- vector("list", B)
#pava_dist <- vector("list", B)
for(i in 1:B){
#x_boot <- sample(x, length(x), replace = T)
#u <- runif(length(x), min = 0, max = 1)
#x_boot <- res[[1]][[2]][sapply(u, function(x) which.min(abs(x  - res[[1]][[1]])))]
samp_x <-   rmultinom(1, size = n_data, probs)
x_boot <- NULL
for(j in 1:length(probs)){
x_boot <- c(x_boot, rep(res[[1]][[2]][j],samp_x[j]))
}
boot_cdf <- dpCDF(x_boot, lower_bound, upper_bound, epsilon, granularity, cdp, num_trials = 1)
#boot_dist[[i]] <- boot_cdf[[1]][[1]]
boot_dist[[i]] <- gpava(boot_cdf[[1]][[2]], boot_cdf[[1]][[1]])$x
lines(res[[1]][[2]], boot_dist[[i]], col = viridis::viridis(3, 0.3)[2])
#pava_cdf <- boot_cdf
#pava_cdf[[1]][[1]] <- gpava(boot_cdf[[1]][[2]], boot_cdf[[1]][[1]])$x
#pava_dist[[i]] <- pava_cdf[[1]][[1]]
#lines(pava_cdf[[1]][[2]], pava_cdf[[1]][[1]], col = viridis::viridis(3, 0.3)[2])
}
32/65
citation(e1071)
citation("e1071")
citation("glmnet")
toBibtex(citation("glmnet"))
x <- rnorm(1000)
y  <- 1 + x + 0.8*x^2 + 0.3*x^3 + rnorm(1000, 0, 2)
df <- data.frame(y, x)
x <- rnorm(1000)
y  <- 1 + x + 0.8*x^2 + 0.3*x^3 + rnorm(1000, 0, 2)
test_set <- data.frame(y, x)
n_folds <- 10
fold <- sample(rep(1:n_folds, 100))
res <- list()
for(i in 1:5){
tmp <- NULL
for(cv in 1:n_folds){
train <- df[fold != cv,]
test <- df[fold == cv, ]
reg <- lm(y~poly(x, i, raw = T), data = train)
pred <- predict(reg, newdata = test)
tmp <- c(tmp, mean((test$y - pred)^2))
}
res[[i]] <- tmp
}
lapply(res, mean)
par(mfrow = c(1, 2))
plot(df$x, df$y, bty = "n", las = 1, ylab = "Y", xlab = "X", col = viridis::viridis(4, 0.4)[1], pch = 19, xlim = c(-4, 4), ylim = c(-6, 26))
reg <- lm(y~poly(x, 1, raw = T), data = df)
q <- seq(-4, 4, 0.01)
lines(q, predict(reg, data.frame(x = q)), col = viridis::viridis(4, 0.7)[2], lwd = 3)
mean((test_set$y - predict(reg, newdata = test_set))^2)
mean((df$y - predict(reg))^2)
reg <- lm(y~poly(x, 3, raw = T), data = df)
q <- seq(-4, 4, 0.01)
lines(q, predict(reg, data.frame(x = q)), col = viridis::viridis(4, 0.7)[3], lwd = 3)
mean((test_set$y - predict(reg, newdata = test_set))^2)
mean((df$y - predict(reg))^2)
reg <- lm(y~poly(x, 25, raw = T), data = df)
q <- seq(-4, 4, 0.01)
lines(q, predict(reg, data.frame(x = q)), col = viridis::viridis(4, 0.7)[4], lwd = 3)
mean((test_set$y - predict(reg, newdata = test_set))^2)
mean((df$y - predict(reg))^2)
plot(test_set$x, test_set$y, bty = "n", las = 1, ylab = "Y", xlab = "X", col = viridis::viridis(4, 0.4)[1], pch = 19, xlim = c(-4, 4), ylim = c(-6, 26))
reg <- lm(y~poly(x, 1, raw = T), data = df)
q <- seq(-4, 4, 0.01)
lines(q, predict(reg, data.frame(x = q)), col = viridis::viridis(4, 0.7)[2], lwd = 3)
mean((test_set$y - predict(reg, newdata = test_set))^2)
mean((df$y - predict(reg))^2)
reg <- lm(y~poly(x, 3, raw = T), data = df)
q <- seq(-4, 4, 0.01)
lines(q, predict(reg, data.frame(x = q)), col = viridis::viridis(4, 0.7)[3], lwd = 3)
mean((test_set$y - predict(reg, newdata = test_set))^2)
mean((df$y - predict(reg))^2)
reg <- lm(y~poly(x, 25, raw = T), data = df)
q <- seq(-4, 4, 0.01)
lines(q, predict(reg, data.frame(x = q)), col = viridis::viridis(4, 0.7)[4], lwd = 3)
mean((test_set$y - predict(reg, newdata = test_set))^2)
mean((df$y - predict(reg))^2)
nrow(df)%/%n_folds
nrow(df)%/%n_folds
set.seed(12102021)
x <- rnorm(1000)
y  <- 1 + x + 0.8*x^2 + 0.3*x^3 + rnorm(1000, 0, 2)
df <- data.frame(y, x)
n_folds <- 10
fold <- sample(rep(1:n_folds, nrow(df)%/%n_folds))
res <- list()
search_grid <- 1:10
for(lambda in search_grid){
tmp <- NULL
for(cv in 1:n_folds){
train <- df[fold != cv,]
test <- df[fold == cv, ]
reg <- lm(y~poly(x, lambda, raw = T), data = train)
pred <- predict(reg, newdata = test)
tmp <- c(tmp, mean((test$y - pred)^2))
}
res[[i]] <- tmp
}
sapply(res, mean)
set.seed(12102021)
x <- rnorm(1000)
y  <- 1 + x + 0.8*x^2 + 0.3*x^3 + rnorm(1000, 0, 2)
df <- data.frame(y, x)
n_folds <- 10
fold <- sample(rep(1:n_folds, nrow(df)%/%n_folds))
res <- list()
search_grid <- 1:10
for(lambda in search_grid){
tmp <- NULL
for(cv in 1:n_folds){
train <- df[fold != cv,]
test <- df[fold == cv, ]
reg <- lm(y~poly(x, lambda, raw = T), data = train)
pred <- predict(reg, newdata = test)
tmp <- c(tmp, mean((test$y - pred)^2))
}
res[[paste0(lambda)]] <- tmp
}
sapply(res, mean)
# Get best hyperparameter setting
sapply(res, mean)
# Get best hyperparameter setting
which.min(sapply(res, mean))
# Get best hyperparameter setting. Here minimal cross validation error.
min(sapply(res, mean))
# Calculate average cross validation error for each value of lambda
avg_cv_error <- sapply(res, mean)
avg_cv_error
avg_cv_error == min(avg_cv_error)
# Get the value of lambda with the minimal average cross validation error
avg_cv_error == min(avg_cv_error)
# Get the value of lambda with the minimal average cross validation error
names(avg_cv_error)[avg_cv_error == min(avg_cv_error)]
# Get the value of lambda with the minimal average cross validation error
as.numeric(names(avg_cv_error)[avg_cv_error == min(avg_cv_error)])
# Get the value of lambda with the minimal average cross validation error
best_lambda <- as.numeric(names(avg_cv_error)[avg_cv_error == min(avg_cv_error)])
# Plot results of linear regression (lambda = 1) and best model
par(mfrow = c(1, 1))
plot(df$x, df$y, bty = "n", las = 1, ylab = "Y", xlab = "X", col = viridis::viridis(4, 0.4)[1], pch = 19, xlim = c(-4, 4), ylim = c(-6, 26))
plot(df$x, df$y, bty = "n", las = 1, ylab = "Y", xlab = "X", col = viridis::viridis(4, 0.4)[1], pch = 19, xlim = c(-4, 4), ylim = c(-6, 26))
reg <- lm(y~poly(x, 1, raw = T), data = df)
q <- seq(-4, 4, 0.01)
lines(q, predict(reg, data.frame(x = q)), col = viridis::viridis(4, 0.7)[2], lwd = 3)
reg <- lm(y~poly(x, 3, raw = T), data = df)
reg <- lm(y~poly(x, best_lambda, raw = T), data = df)
q <- seq(-4, 4, 0.01)
lines(q, predict(reg, data.frame(x = q)), col = viridis::viridis(4, 0.7)[3], lwd = 3)
?dev.copy
dev.copy(png("~/Downloads/test.png", width = 695, height = 450))
dev.copy(png, "~/Downloads/test.png", width = 695, height = 450)
dev.copy(png, filename = "~/Downloads/test.png", width = 695, height = 450)
dev.off()
dev.off()
dev.off()
dev.off()
plot(df$x, df$y, bty = "n", las = 1, ylab = "Y", xlab = "X", col = viridis::viridis(4, 0.4)[1], pch = 19, xlim = c(-4, 4), ylim = c(-6, 26))
reg <- lm(y~poly(x, 1, raw = T), data = df)
q <- seq(-4, 4, 0.01)
lines(q, predict(reg, data.frame(x = q)), col = viridis::viridis(4, 0.7)[2], lwd = 3)
# Retrain model with best lambda on entire training data
reg <- lm(y~poly(x, best_lambda, raw = T), data = df)
lines(q, predict(reg, data.frame(x = q)), col = viridis::viridis(4, 0.7)[3], lwd = 3)
dev.copy(png, filename = "~/Downloads/test.png", width = 695, height = 450)
dev.off()
# Do you want to save the plot to disk at the end?
save_plot <- TRUE
plot_path <-  "~/Downloads/bivariate_example.png"
# Set seed for replicability (seed is date of our last run)
set.seed(12102021)
# Generate the true data
x <- rnorm(1000)
y  <- 1 + x + 0.8 * x ^ 2 + 0.3 * x ^ 3 + rnorm(1000, 0, 2)
df <- data.frame(y, x)
# Set the number of folds for cross validation
n_folds <- 10
# Generate fold ids
fold <- sample(rep(1:n_folds, nrow(df) %/% n_folds))
# Set up search grid. Here with only lambda to tune it is just a vector
search_grid <- 1:10
# Initialize object to collect results
res <- list()
for (lambda in search_grid) {
# Initialize object for results of one hyperparameter setting
tmp <- NULL
# Start cross validation loop
for (cv in 1:n_folds) {
# Subset data to training folds and test fold
train <- df[fold != cv, ]
test <- df[fold == cv,]
# Train model with current hyperparameter setting
reg <- lm(y ~ poly(x, lambda, raw = T), data = train)
# Make prediction with trained model on test set
pred <- predict(reg, newdata = test)
# Calculate error and store results
tmp <- c(tmp, mean((test$y - pred) ^ 2))
}
# Store results of run in res object
res[[paste0(lambda)]] <- tmp
}
# Calculate average cross validation error for each value of lambda
avg_cv_error <- sapply(res, mean)
# Get the value of lambda with the minimal average cross validation error
best_lambda <- as.numeric(names(avg_cv_error)[avg_cv_error == min(avg_cv_error)])
plot(df$x, df$y, bty = "n", las = 1, ylab = "Y", xlab = "X", col = viridis::viridis(4, 0.4)[1], pch = 19, xlim = c(-4, 4), ylim = c(-6, 26))
reg <- lm(y~poly(x, 1, raw = T), data = df)
q <- seq(-4, 4, 0.01)
lines(q, predict(reg, data.frame(x = q)), col = viridis::viridis(4, 0.7)[2], lwd = 3)
# Retrain model with best lambda on entire training data
reg <- lm(y~poly(x, best_lambda, raw = T), data = df)
lines(q, predict(reg, data.frame(x = q)), col = viridis::viridis(4, 0.7)[3], lwd = 3)
if(save_plot){
dev.copy(png, filename = plot_path, width = 695, height = 450)
dev.off()
}
